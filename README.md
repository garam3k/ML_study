# ML study
기존의 머신러닝은 결과가 왜 잘 나오는지 과정을 분석할 수 있었다.
하지만 딥러닝 영역으로 넘어오면서 결과는 잘 나오지만 왜 그런지 해석할 수 없는 부분이 있다.

https://youtu.be/6W6PVWByhc0

이 영상을 바탕으로 한번 정리해 보려고 한다.
딥러닝에 관한 intuition을 정리할 수 있었다.

## 1. 딥러닝 네트워크가 커질수록 성능이 잘 나온다.
인간의 뇌에는 10^11개의 뉴런이 있다고 한다.
최근 NLP에서 핫한 GPT-3의 경우 12-billion 개의 파라미터가 있다고한다.
뉴런과 파라미터는 다르지만 (일반적으로 뉴런 하나당 여러개의 파라미터를 할당한다) 잘 작동하는 네트워크 = 큰 규모의 네트워크 라는것이 일반적으로 적용된다.

## 2. 간단한 optimization으로도 global optima에 도달한다.
대규모의 네트워크는 매우 복잡한 구조로 되어있고 복잡할수록 cost function이 convex하지 않을 가능성이 높다는게 일반적인 상식이다.
하지만 딥러닝은 간단한 SGD로도 결과가 잘 나오는 parameter를 학습할 수 있다.
네트워크를 학습할때 parameter는 랜덤하게 초기화되는데 새로 학습을하면 다른 값으로 최적화가 되는데 성능은 비슷하게 나오는경우가 많다.
이 논문에 따르면 local optima = global optima이고 연속적으로 이어져있다고 한다.
이것도 증명은 못하지만 딥러닝에서 일반적으로 local optima = global optima 라고 생각할 수 있다.

## 3. 딥러닝은 overfit하지 않는다.
polynomial regression의 경우 차수가 높아질수록 과적합 하게된다.
그래서 네트워크가 깊어질수록(커질수록) 과적합되는게 맞는것 같지만,
실제로 결과들은 
